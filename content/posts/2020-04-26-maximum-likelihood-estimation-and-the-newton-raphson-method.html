---
title: Maximum Likelihood Estimation and the Newton-Raphson method
author: Jorge Lopez
date: '2020-04-26'
slug: maximum-likelihood-estimation-and-the-newton-raphson-method
categories:
  - rstats
tags:
  - mle
---



<p>The Maximum Likelihood Estimation (MLE) is probably one of the most well-known methods for estimating the parameters of a particular statistical model, given the data. It aims at finding the parameter values that makes the observed data most likely under the assumed statistical model.</p>
<p>Let <span class="math inline">\(X_1, ... X_n\)</span> be independent and identically distributed random variables. We assume that the data are drawn from a distribution with density <span class="math inline">\(f(x|\theta)\)</span>. Given the data, we define the likelihood as follows:</p>
<center>
<span class="math inline">\(L(\theta) = \prod_{i=1}^{N}f(x_i|\theta)\)</span>
</center>
<p>Then the goal of MLE is to find the parameters values that maximize the likelihood function, given the data.</p>
<center>
<span class="math inline">\(\widehat{\theta} = argmax_{\theta} \widehat{L}(\theta;x)\)</span>
</center>
<p>I won’t go into the details but it’s quite common to take logarithms on the likelihood function (since the logarithm is a monotonically increasing function) so the maximization of the log-likelihood is equivalent to maximize the likelihood function itself and it ends up being easier to compute because the product becomes a sum.</p>
<div id="newton-rapshon-method" class="section level3">
<h3>Newton-Rapshon method</h3>
<p>Some methods have been developed to cope with the solution when the analytical solution is hard to compute, which is true most of the times. One of them is the <a href="https://en.wikipedia.org/wiki/Newton%27s_method"><em>Newton-Rapshon algorithm</em></a>, developed by <a href="https://en.wikipedia.org/wiki/Isaac_Newton">Isaac Newton</a> and <a href="https://en.wikipedia.org/wiki/Joseph_Raphson">Joseph Raphson</a>, which is used to approximate roots of real-valued functions.</p>
<p>We start with a initial values, <span class="math inline">\(x_0\)</span>. Then, if the function satisfies the proper assumptions,</p>
<center>
<span class="math inline">\(x_{1} = x_0 - \frac{f(x_0)}{f&#39;(x_0)}\)</span>
</center>
<p>is a better approximation of the root than <span class="math inline">\(x_0\)</span>. <span class="math inline">\((x_1, 0)\)</span> is the intersection of the x-axis and the tangent of the function at <span class="math inline">\((x_0, f(x_0))\)</span>. In the following plot our first guess <span class="math inline">\((x_0)\)</span> is 38 and after drawing the tangent line we see that the intersection with the x-axis is at 25.3, which becomes the <span class="math inline">\(x_1\)</span> value.</p>
<p><img src="/posts/2020-04-26-maximum-likelihood-estimation-and-the-newton-raphson-method_files/plot_zoom_png.png" /></p>
<p>The process will be repeated as</p>
<center>
<span class="math inline">\(x_{n+1} = x_n - \frac{f(x_n)}{f&#39;(x_n)}\)</span>
</center>
<p>until the difference between <span class="math inline">\(x_{n+1}\)</span> and <span class="math inline">\(x_n\)</span> is smaller than a tiny value (it’s called <em>epsilon</em> in this post).</p>
<p>Let’s see an example with a random function and then let’s see the application for the MLE case. Suppose we have this function (this is the function plotted above):</p>
<center>
<span class="math inline">\(f(x) = 2x^3 + x^2 + 10\)</span>
</center>
<p>We also know that:</p>
<center>
<span class="math inline">\(f&#39;(x) = 6x^2 + 2x\)</span>
</center>
<p>Let’s jump to the code. Define the function and its derivative:</p>
<pre class="r"><code># Function
func &lt;- function(x) 2 * x^3 + x^2 + 10

# Derivative function
dfunc &lt;- function(x) 6 * x^2 + 2*x</code></pre>
<p>Let’s see how the code for the Newton-Raphsonm method looks like and run it:</p>
<pre class="r"><code>newton_raphson &lt;- 
  function(xn, epsilon = 1e-6, n = 500) {
    
    # store all values
    values &lt;- xn
    
    # loop n times (in this example we&#39;ll never reach the max number of iterations)
    for (i in seq_len(n)) {
      
      # NR equation
      xn1 &lt;- xn - func(xn)/dfunc(xn)
      cat(&quot;Iteration&quot;, i, &quot;Value&quot;, xn1, &quot;\n&quot;)
      
      # accumulative iteration values
      values &lt;- c(values, xn1)
      
      # if difference between xn1 and xn is less than epsilon, convergence reached
      if(abs(xn1 - xn) &lt; epsilon) {
        cat(&quot;Convergence reached!&quot;, &quot;\n&quot;)
        res &lt;- list(final_value = values[length(values)],
                    values = values)
        return(res)
      }
      
      # new iteration
      xn &lt;- xn1
    }
  }


tst &lt;- newton_raphson(xn = 38)</code></pre>
<pre><code>## Iteration 1 Value 25.27712 
## Iteration 2 Value 16.794 
## Iteration 3 Value 11.13573 
## Iteration 4 Value 7.356832 
## Iteration 5 Value 4.821948 
## Iteration 6 Value 3.095622 
## Iteration 7 Value 1.856579 
## Iteration 8 Value 0.7806913 
## Iteration 9 Value -1.434821 
## Iteration 10 Value -2.083476 
## Iteration 11 Value -1.912195 
## Iteration 12 Value -1.894124 
## Iteration 13 Value -1.893932 
## Iteration 14 Value -1.893932 
## Convergence reached!</code></pre>
<p>This is the final value and all the previous ones, <span class="math inline">\(x_0,...,x_{15}\)</span>:</p>
<pre class="r"><code>tst</code></pre>
<pre><code>## $final_value
## [1] -1.893932
## 
## $values
##  [1] 38.0000000 25.2771167 16.7940041 11.1357341  7.3568321  4.8219479
##  [7]  3.0956224  1.8565792  0.7806913 -1.4348208 -2.0834756 -1.9121949
## [13] -1.8941237 -1.8939316 -1.8939315</code></pre>
<p>And this would be the whole iterations process:</p>
<p><img src="/posts/2020-04-26-maximum-likelihood-estimation-and-the-newton-raphson-method_files/animation_1.gif" /></p>
</div>
<div id="poisson-example" class="section level3">
<h3>Poisson example</h3>
<p>What about a “real life example”? Assume a Poisson distribution with probability mass function:</p>
<center>
<span class="math inline">\(P(X = x) = \frac{\lambda^x e^{-\lambda}}{x!}\)</span>
</center>
<p>For <span class="math inline">\(X_1,...,X_n\)</span> identically distributed random variables we have a likelihood function like:</p>
<center>
<span class="math inline">\(L_\lambda = \prod_{i=1}^{N}\frac{\lambda^x e^{-\lambda}}{x!}\)</span>
</center>
<p>
</p>
<center>
<span class="math inline">\(L_\lambda = \frac{\lambda^{\sum x_i} e^{-n\lambda}}{\prod x!}\)</span>
</center>
<p>Taking logs and reordering:</p>
<center>
<span class="math inline">\(l_\lambda = log(\lambda)\sum x_i - n\lambda -\sum log(x_i!)\)</span>
</center>
<p>Now we find the maximum likelihood estimation by taking the derivative and equaling it to zero. This lead us to conclude that the MLE is equal to the mean:</p>
<center>
<span class="math inline">\(l&#39;_\lambda = \frac{1}{\lambda} \sum x_i - n = 0 \Leftrightarrow \lambda = \frac{\sum x_i}{n} = \overline{X}\)</span>
</center>
<p>Going back to the code, let’s now simulate 100 observations from a Poisson distribution with <span class="math inline">\(\lambda = 4\)</span>:</p>
<pre class="r"><code>set.seed(123)

# Simulate 100 random values
df &lt;- rpois(n = 100, lambda = 4)

# Mean?
mean(df)</code></pre>
<pre><code>## [1] 4.09</code></pre>
<pre class="r"><code># Probability mass function for the Poisson process
func &lt;- function(x) {
  sum(df)/x - 100
}

# Derivative from the previous function
dfunc &lt;- function(x) {
  -sum(df)/x^2
  }</code></pre>
<p>Now, I’m going to run the Newton-Raphson method choosing <span class="math inline">\(\lambda = 7\)</span> as starting value:</p>
<pre class="r"><code>tst &lt;- newton_raphson(xn = 7)</code></pre>
<pre><code>## Iteration 1 Value 2.01956 
## Iteration 2 Value 3.041902 
## Iteration 3 Value 3.821416 
## Iteration 4 Value 4.072362 
## Iteration 5 Value 4.089924 
## Iteration 6 Value 4.09 
## Iteration 7 Value 4.09 
## Convergence reached!</code></pre>
<p><img src="/posts/2020-04-26-maximum-likelihood-estimation-and-the-newton-raphson-method_files/animation.gif" /></p>
<p>Great, the mean value is reached!</p>
<p>Unfortunately, as usual, there’s no free lunch. What if we had chosen a bad starting point? Well, the algorithm fail to converge and crash.</p>
<pre class="r"><code>tst &lt;- newton_raphson(xn = 15)</code></pre>
<pre><code>## Iteration 1 Value -25.01222 
## Iteration 2 Value -202.9857 
## Iteration 3 Value -10480.1 
## Iteration 4 Value -26874867 
## Iteration 5 Value -1.765914e+14 
## Iteration 6 Value -7.624575e+27 
## Iteration 7 Value -1.421373e+55 
## Iteration 8 Value -4.939609e+109 
## Iteration 9 Value -5.965707e+218 
## Iteration 10 Value -Inf 
## Iteration 11 Value -Inf</code></pre>
<pre><code>## Error in if (abs(xn1 - xn) &lt; epsilon) {: valor ausente donde TRUE/FALSE es necesario</code></pre>
<p>I’ve seen there are <a href="https://www.quora.com/How-do-you-avoid-a-bad-starting-point-in-Newtons-method">some</a> mathematical approaches to deal with this, but maybe in the next chapter!</p>
</div>
